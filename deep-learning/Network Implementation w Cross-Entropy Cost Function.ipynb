{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a simple network that can distinguish MNIST hand written digits\n",
    "\n",
    "based on Michael Neilson's nn book: http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to modify the Network class to accept a different cost function, namely cross-entry cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray} \n",
    "  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right]\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks complicated by when we compute dC/dw, it simplifies to avoid the sigmoid_prime term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "      \\frac{\\partial C}{\\partial w^L_{jk}} & = & \\frac{1}{n} \\sum_x \n",
    "      a^{L-1}_k  (a^L_j-y_j) \\\\\n",
    "      \\frac{\\partial C}{\\partial b^L_{j}} & = & \\frac{1}{n} \\sum_x \n",
    "      (a^L_j-y_j).\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy Cost function is introducted to avoid the slow down of learning caused by the near zero value of the derivative of sigmoid function when z is either very negative or very positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data set\n",
    "def load_data(file_loc):\n",
    "    with gzip.open(file_loc, 'rb') as file:\n",
    "        training_data, validation_data, test_data = pickle.load(file, encoding='latin1')\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_result(y):\n",
    "    e = np.zeros((10,1))\n",
    "    e[y] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(file_loc):\n",
    "    tr_data, val_data, test_data = load_data(file_loc)\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_data[0]]\n",
    "    training_results = [vectorize_result(y) for y in tr_data[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    \n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in val_data[0]]\n",
    "    validation_data = list(zip(validation_inputs, val_data[1]))\n",
    "    \n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
    "    test_data = list(zip(test_inputs, test_data[1]))\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost:\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn():\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y) * sigmoid_prime(z)\n",
    "    \n",
    "class CrossEntropyCost:\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn():\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y)\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, sizes, cost = QuadraticCost):\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.weights = [np.random.randn(y, x) for x, y in list(zip(sizes[:-1], sizes[1:]))]\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.cost = cost\n",
    "    \n",
    "    def feed_fwd(self, a):\n",
    "        for w, b in list(zip(self.weights, self.biases)):\n",
    "            a = sigmoid(np.dot(w,a) + b)\n",
    "        return a\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feed_fwd(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    def SGD(self, train_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        n = len(train_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(train_data)\n",
    "            mini_batches = [train_data[k: k + mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)}, {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j}\")\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        m = len(mini_batch)\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        for x, y in mini_batch:\n",
    "            # delta_nabla_w is nabla_w for each training example x derived using backprop\n",
    "            delta_nabla_w, delta_nabla_b = self.back_prop(x, y)\n",
    "            \n",
    "            # then we accumulate the nablas contributed by all the training examples in the mini_batch in nabla_w\n",
    "            nabla_w = [nw + dnw for nw, dnw in list(zip(nabla_w, delta_nabla_w))]\n",
    "            nabla_b = [nb + dnb for nb, dnb in list(zip(nabla_b, delta_nabla_b))]\n",
    "        self.weights = [w - (eta/m) * nw for w, nw in list(zip(self.weights, nabla_w))]\n",
    "        self.biases  = [b - (eta/m) * nb for b, nb in list(zip(self.biases, nabla_b))]\n",
    "    \n",
    "    def back_prop(self, x, y):\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        # feed fwd\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for w, b in list(zip(self.weights, self.biases)):\n",
    "            z = np.dot(w, activations[-1]) + b\n",
    "            zs.append(z)\n",
    "            activations.append(sigmoid(z))\n",
    "            \n",
    "        # backward pass\n",
    "        delta = self.cost.delta(zs[-1], activations[-1], y)\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            nabla_b[-l] = delta\n",
    "        return (nabla_w, nabla_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = load_mnist('../data/mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8102, 10000\n",
      "Epoch 1: 8320, 10000\n",
      "Epoch 2: 9336, 10000\n",
      "Epoch 3: 9339, 10000\n",
      "Epoch 4: 9403, 10000\n",
      "Epoch 5: 9424, 10000\n",
      "Epoch 6: 9452, 10000\n",
      "Epoch 7: 9436, 10000\n",
      "Epoch 8: 9460, 10000\n",
      "Epoch 9: 9465, 10000\n",
      "Took 53.5204873085022\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time() \n",
    "net = Network([784, 30, 10])\n",
    "net.SGD(train_data, 10, 10, 3.0, test_data = test_data)\n",
    "time2 = time.time()\n",
    "print(f\"Took {time2 - time1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from Michael Nielson\n",
    "\n",
    "There is, incidentally, a very rough general heuristic for relating the learning rate for the cross-entropy and the quadratic cost. As we saw earlier, the gradient terms for the quadratic cost have an extra σ′=σ(1−σ) term in them. Suppose we average this over values for σ, ∫10dσσ(1−σ)=1/6. We see that (very roughly) the quadratic cost learns an average of 6 times slower, for the same learning rate. This suggests that a reasonable starting point is to divide the learning rate for the quadratic cost by 6. Of course, this argument is far from rigorous, and shouldn't be taken too seriously. Still, it can sometimes be a useful starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9153, 10000\n",
      "Epoch 1: 9251, 10000\n",
      "Epoch 2: 9280, 10000\n",
      "Epoch 3: 9358, 10000\n",
      "Epoch 4: 9357, 10000\n",
      "Epoch 5: 9385, 10000\n",
      "Epoch 6: 9404, 10000\n",
      "Epoch 7: 9425, 10000\n",
      "Epoch 8: 9452, 10000\n",
      "Epoch 9: 9455, 10000\n",
      "Took 49.707035064697266\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time() \n",
    "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "net.SGD(train_data, 10, 10, .5, test_data = test_data) # Set the learning rate lower\n",
    "time2 = time.time()\n",
    "print(f\"Took {time2 - time1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
